"""
æ•ˆèƒ½åŸºæº–æ¸¬è©¦è…³æœ¬

ç”¨æ–¼æ¯”è¼ƒç•¶å‰æ¶æ§‹ vs 2025 å„ªåŒ–æ¶æ§‹çš„æ•ˆèƒ½å·®ç•°
"""

import asyncio
import time
from dataclasses import dataclass
from typing import List, Dict, Any
import statistics
import json
from datetime import datetime
import structlog

logger = structlog.get_logger()


@dataclass
class BenchmarkResult:
    """åŸºæº–æ¸¬è©¦çµæœ"""
    test_name: str
    architecture: str  # "current" or "optimized"
    latency_p50: float
    latency_p95: float
    latency_p99: float
    avg_latency: float
    min_latency: float
    max_latency: float
    throughput: float  # requests per second
    error_rate: float
    cost_per_call: float


class PerformanceBenchmark:
    """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""

    def __init__(self):
        self.results: List[BenchmarkResult] = []

    async def benchmark_latency(
        self,
        handler: callable,
        test_name: str,
        architecture: str,
        num_calls: int = 100
    ) -> BenchmarkResult:
        """
        åŸºæº–æ¸¬è©¦å»¶é²

        Args:
            handler: é€šè©±è™•ç†å‡½æ•¸
            test_name: æ¸¬è©¦åç¨±
            architecture: æ¶æ§‹é¡å‹
            num_calls: æ¸¬è©¦é€šè©±æ•¸é‡

        Returns:
            åŸºæº–æ¸¬è©¦çµæœ
        """
        logger.info(
            "starting_benchmark",
            test=test_name,
            arch=architecture,
            calls=num_calls
        )

        latencies = []
        errors = 0
        start_time = time.time()

        for i in range(num_calls):
            call_start = time.time()

            try:
                await handler(call_id=f"test-{i}")
                latency = (time.time() - call_start) * 1000  # ms
                latencies.append(latency)

            except Exception as e:
                errors += 1
                logger.error("call_failed", error=str(e), call_id=i)

            # é¿å…éè¼‰
            if i % 10 == 0:
                await asyncio.sleep(0.1)

        total_time = time.time() - start_time

        # è¨ˆç®—çµ±è¨ˆ
        latencies.sort()
        result = BenchmarkResult(
            test_name=test_name,
            architecture=architecture,
            latency_p50=self._percentile(latencies, 50),
            latency_p95=self._percentile(latencies, 95),
            latency_p99=self._percentile(latencies, 99),
            avg_latency=statistics.mean(latencies) if latencies else 0,
            min_latency=min(latencies) if latencies else 0,
            max_latency=max(latencies) if latencies else 0,
            throughput=num_calls / total_time,
            error_rate=errors / num_calls,
            cost_per_call=self._estimate_cost(architecture)
        )

        self.results.append(result)

        logger.info(
            "benchmark_completed",
            test=test_name,
            arch=architecture,
            p50=f"{result.latency_p50:.2f}ms",
            p95=f"{result.latency_p95:.2f}ms",
            throughput=f"{result.throughput:.2f} req/s"
        )

        return result

    def _percentile(self, data: List[float], percentile: int) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        if not data:
            return 0.0
        size = len(data)
        index = (size * percentile) // 100
        return data[min(index, size - 1)]

    def _estimate_cost(self, architecture: str) -> float:
        """ä¼°ç®—æ¯é€šé›»è©±æˆæœ¬"""
        if architecture == "current":
            # ç•¶å‰æ¶æ§‹æˆæœ¬ä¼°ç®—
            return (
                0.005 +   # STT (gpt-4o-transcribe)
                0.08 +    # LLM (gpt-4o-nano, ~1000 tokens)
                0.005 +   # TTS (Azure Neural)
                0.01      # å…¶ä»–æœå‹™ (Cosmos, Redis, etc.)
            )
        else:  # optimized
            # å„ªåŒ–æ¶æ§‹æˆæœ¬ä¼°ç®—
            return (
                0.10 +    # Realtime API (ç«¯åˆ°ç«¯)
                0.005     # å…¶ä»–æœå‹™ (æˆæœ¬é™ä½)
            )

    def compare_results(self) -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒæ¶æ§‹çš„çµæœ"""
        comparison = {
            "timestamp": datetime.now().isoformat(),
            "architectures": {},
            "improvements": {}
        }

        # æŒ‰æ¶æ§‹åˆ†çµ„
        by_arch = {}
        for result in self.results:
            if result.architecture not in by_arch:
                by_arch[result.architecture] = []
            by_arch[result.architecture].append(result)

        # è¨ˆç®—å¹³å‡å€¼
        for arch, results in by_arch.items():
            comparison["architectures"][arch] = {
                "avg_latency_p50": statistics.mean([r.latency_p50 for r in results]),
                "avg_latency_p95": statistics.mean([r.latency_p95 for r in results]),
                "avg_latency_p99": statistics.mean([r.latency_p99 for r in results]),
                "avg_throughput": statistics.mean([r.throughput for r in results]),
                "avg_error_rate": statistics.mean([r.error_rate for r in results]),
                "avg_cost_per_call": statistics.mean([r.cost_per_call for r in results])
            }

        # è¨ˆç®—æ”¹é€²ç™¾åˆ†æ¯”
        if "current" in comparison["architectures"] and "optimized" in comparison["architectures"]:
            current = comparison["architectures"]["current"]
            optimized = comparison["architectures"]["optimized"]

            comparison["improvements"] = {
                "latency_p50_reduction": self._calc_improvement(
                    current["avg_latency_p50"],
                    optimized["avg_latency_p50"]
                ),
                "latency_p95_reduction": self._calc_improvement(
                    current["avg_latency_p95"],
                    optimized["avg_latency_p95"]
                ),
                "latency_p99_reduction": self._calc_improvement(
                    current["avg_latency_p99"],
                    optimized["avg_latency_p99"]
                ),
                "throughput_increase": self._calc_improvement(
                    optimized["avg_throughput"],
                    current["avg_throughput"],
                    inverse=True
                ),
                "cost_reduction": self._calc_improvement(
                    current["avg_cost_per_call"],
                    optimized["avg_cost_per_call"]
                )
            }

        return comparison

    def _calc_improvement(
        self,
        before: float,
        after: float,
        inverse: bool = False
    ) -> float:
        """è¨ˆç®—æ”¹é€²ç™¾åˆ†æ¯”"""
        if before == 0:
            return 0.0

        if inverse:
            # å°æ–¼ååé‡ç­‰æŒ‡æ¨™ï¼ˆæ•¸å€¼è¶Šé«˜è¶Šå¥½ï¼‰
            return ((after - before) / before) * 100
        else:
            # å°æ–¼å»¶é²ã€æˆæœ¬ç­‰æŒ‡æ¨™ï¼ˆæ•¸å€¼è¶Šä½è¶Šå¥½ï¼‰
            return ((before - after) / before) * 100

    def generate_report(self, output_file: str = "benchmark_report.json"):
        """ç”Ÿæˆå ±å‘Š"""
        comparison = self.compare_results()

        # æ§åˆ¶å°è¼¸å‡º
        print("\n" + "=" * 80)
        print("ğŸ“Š æ•ˆèƒ½åŸºæº–æ¸¬è©¦å ±å‘Š")
        print("=" * 80)

        for arch, metrics in comparison["architectures"].items():
            print(f"\nğŸ—ï¸  {arch.upper()} æ¶æ§‹:")
            print(f"  â€¢ P50 å»¶é²: {metrics['avg_latency_p50']:.2f}ms")
            print(f"  â€¢ P95 å»¶é²: {metrics['avg_latency_p95']:.2f}ms")
            print(f"  â€¢ P99 å»¶é²: {metrics['avg_latency_p99']:.2f}ms")
            print(f"  â€¢ ååé‡: {metrics['avg_throughput']:.2f} req/s")
            print(f"  â€¢ éŒ¯èª¤ç‡: {metrics['avg_error_rate']*100:.2f}%")
            print(f"  â€¢ æ¯é€šæˆæœ¬: ${metrics['avg_cost_per_call']:.4f}")

        if comparison["improvements"]:
            print("\nğŸ“ˆ æ”¹é€²æŒ‡æ¨™:")
            improvements = comparison["improvements"]
            print(f"  â€¢ P50 å»¶é²é™ä½: {improvements['latency_p50_reduction']:.1f}%")
            print(f"  â€¢ P95 å»¶é²é™ä½: {improvements['latency_p95_reduction']:.1f}%")
            print(f"  â€¢ P99 å»¶é²é™ä½: {improvements['latency_p99_reduction']:.1f}%")
            print(f"  â€¢ ååé‡æå‡: {improvements['throughput_increase']:.1f}%")
            print(f"  â€¢ æˆæœ¬é™ä½: {improvements['cost_reduction']:.1f}%")

        print("\n" + "=" * 80)

        # è¼¸å‡º JSON æª”æ¡ˆ
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)

        logger.info("report_generated", file=output_file)


# æ¸¬è©¦è™•ç†å™¨ï¼ˆæ¨¡æ“¬ï¼‰
async def mock_current_handler(call_id: str):
    """æ¨¡æ“¬ç•¶å‰æ¶æ§‹çš„é€šè©±è™•ç†"""
    # æ¨¡æ“¬å»¶é²
    await asyncio.sleep(0.5 + (time.time() % 0.5))  # 500-1000ms


async def mock_optimized_handler(call_id: str):
    """æ¨¡æ“¬å„ªåŒ–æ¶æ§‹çš„é€šè©±è™•ç†"""
    # æ¨¡æ“¬å»¶é²
    await asyncio.sleep(0.2 + (time.time() % 0.1))  # 200-300ms


# ä¸»æ¸¬è©¦å‡½æ•¸
async def main():
    """åŸ·è¡ŒåŸºæº–æ¸¬è©¦"""
    benchmark = PerformanceBenchmark()

    # æ¸¬è©¦ 1: å»¶é²æ¯”è¼ƒ
    print("ğŸ§ª æ¸¬è©¦ 1: å»¶é²æ¯”è¼ƒ...")

    await benchmark.benchmark_latency(
        handler=mock_current_handler,
        test_name="latency_test",
        architecture="current",
        num_calls=100
    )

    await benchmark.benchmark_latency(
        handler=mock_optimized_handler,
        test_name="latency_test",
        architecture="optimized",
        num_calls=100
    )

    # æ¸¬è©¦ 2: é«˜è² è¼‰æ¸¬è©¦
    print("\nğŸ§ª æ¸¬è©¦ 2: é«˜è² è¼‰æ¸¬è©¦...")

    await benchmark.benchmark_latency(
        handler=mock_current_handler,
        test_name="load_test",
        architecture="current",
        num_calls=50
    )

    await benchmark.benchmark_latency(
        handler=mock_optimized_handler,
        test_name="load_test",
        architecture="optimized",
        num_calls=50
    )

    # ç”Ÿæˆå ±å‘Š
    benchmark.generate_report()


if __name__ == "__main__":
    asyncio.run(main())
